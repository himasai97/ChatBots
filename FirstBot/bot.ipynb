{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the libraries are not installed\n",
    "!pip install tflearn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\himas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\himas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:tensorflow:From C:\\Users\\himas\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "#import the intents file\n",
    "import json\n",
    "with open('intents.json') as intents_file:\n",
    "    intents = json.load(intents_file)\n",
    "\n",
    "# for saving data\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "# Organize the words in each pattern array of the intents dictionary into:\n",
    "            doc: a document array containg the processed words along with their tag\n",
    "            words: an array of all the processed words present in the document\n",
    "            classes: an array of all tags present in the document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90 documents\n14 classes ['greeting', 'goodbye', 'thanks', 'STEMforall', 'history', 'partners', 'contact', 'donations', 'volunteer', 'overview_of_positions', 'lead', 'assistant', 'tutor_application', 'enrollment']\n76 unique stemmed words ['about', 'achieve', 'achievement', 'an', 'anyone', 'application', 'apply', 'applying', 'are', 'assistant', 'available', 'awesome', 'bye', 'chatting', 'class', 'contact', 'course', 'day', 'donate', 'donating', 'donation', 'enroll', 'enrolling', 'far', 'get', 'give', 'good', 'goodbye', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'info', 'information', 'instructor', 'interested', 'later', 'lead', 'main', 'more', 'next', 'nice', 'offered', 'option', 'organization', 'overview', 'partner', 'position', 'provide', 'requirement', 'see', 'service', 'sign', 'sign-up', 'signing', 'sponsor', 'sponsored', 'stemforall', 'support', 'ta', 'teach', 'teacher', 'teaching', 'tell', 'thank', 'thanks', 'till', 'time', 'tutor', 'volunteer', 'volunteering', 'work']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "doc = []\n",
    "# create a list of words that should be ignored as they do not add value to the model\n",
    "ignore_words = ['?','!',',',\"'s\",\".\",\"where\",\"who\",\"is\",\"the\",\"this\",\"in\",\"you\",\"so\",\"for\",\"me\",\"how\",\"want\",\n",
    "                \"your\", \"a\",\"that\",\"there\",\"to\",\"of\",\"i\",\"and\",\"what\",\"with\",\"up\",\"do\",\"did\",\"be\",\"can\",\"could\"]\n",
    "\n",
    "#loop through each pattern in the intents dictionary\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word in the sentence\n",
    "        token_w = nltk.word_tokenize(pattern)\n",
    "        #lemmatize and lower each word if it is not one among the ignore_words array\n",
    "        w = [lemmatizer.lemmatize(word.lower()) for word in token_w if lemmatizer.lemmatize(word.lower()) not in ignore_words]\n",
    "        # add to the words list\n",
    "        words.extend(w)\n",
    "        #add to doc in the corpus\n",
    "        doc.append((w, intent['tag']))\n",
    "        #add the tag to the classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# remove duplicates in the words list\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print (len(doc),'documents')\n",
    "print (len(classes),'classes', classes)\n",
    "print (len(words), 'unique stemmed words', words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n"
   ]
  },
  {
   "source": [
    "# Transform from doc of words to tensors of numbers\n",
    "\n",
    "Once we have our training data, the next step is to transform it into a format that is recognized by the model. In this case, the data in the doc is transformed such that:\n",
    "    \n",
    "    input: bag_of_words - a bag of words tensor of zeros and ones, \n",
    "    \n",
    "          where one indicates that the corresponding word in the words array is present in this pattern and zero, otherwise.\n",
    "    \n",
    "    output: output_vec - an output vector tensor of zeros and ones, \n",
    "    \n",
    "          where one indicates that the corresponding class in the classes array is the output for the given bag of words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training data created\n\ntrain_X example: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\ntrain_y example: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# create empty arrays for storing data\n",
    "train_data = []\n",
    "output = []\n",
    "# Initialize an empty vector for the output\n",
    "empty_vec = [0]*len(classes)\n",
    "\n",
    "# fill training data by obtaining bag of words for each sentence in doc\n",
    "for sent in doc:\n",
    "    #initialize the bag of words array\n",
    "    bag_of_words = []\n",
    "    # get the list of lemmatized words \n",
    "    lemmatized_words = sent[0]\n",
    "    # create bag of words array\n",
    "    for word in words:\n",
    "        bag_of_words.append(1) if word in lemmatized_words else bag_of_words.append(0)\n",
    "\n",
    "    # create output vector such that it has '1' for current tag and '0' for the remaining tags\n",
    "    output_vec = list(empty_vec)\n",
    "    output_vec[classes.index(sent[1])] = 1\n",
    "\n",
    "    train_data.append([bag_of_words, output_vec])\n",
    "\n",
    "# shuffle the features and transform it to an np.array\n",
    "random.shuffle(train_data)\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "#create the X (patterns), y (intents) lists for the model\n",
    "train_X = list(train_data[:,0])\n",
    "train_y = list(train_data[:,1])\n",
    "\n",
    "print('training data created\\n')\n",
    "example_idx = random.randint(0,len(doc))\n",
    "print('train_X example:', train_X[example_idx])\n",
    "print('train_y example:', train_y[example_idx])"
   ]
  },
  {
   "source": [
    "# Building the neural network and training the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# build the neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_X[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "#define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "#train the model (applying gradient descent algorithm)\n",
    "model.fit(train_X, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "\n",
    "model.save('model.tflearn')"
   ]
  }
 ]
}
